// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';
import { APIPromise } from '../core/api-promise';
import { RequestOptions } from '../internal/request-options';
// Custom code: Add streaming support
import { Stream } from '../core/streaming';
import type { APIResponseProps } from '../internal/parse';

// Custom code: Chat completion chunk type for streaming
export interface ChatCompletionChunk {
  id: string;
  object: 'chat.completion.chunk';
  created: number;
  model: string;
  choices: Array<ChatCompletionChunk.Choice>;
  system_fingerprint?: string | null;
  usage?: ChatCompletionsResponse.Usage | null;
}

export namespace ChatCompletionChunk {
  export interface Choice {
    index: number;
    delta: Choice.Delta;
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call' | null;
  }

  export namespace Choice {
    export interface Delta {
      role?: 'assistant' | 'tool';
      content?: string | null;
      refusal?: null;
    }
  }
}

export class Chat extends APIResource {
  /**
   * Create a chat completion (OpenAI compatible).
   * When stream=true, returns a Stream that can be iterated with for-await-of.
   */
  completions(
    body: ChatCompletionsParams & { stream?: false },
    options?: RequestOptions,
  ): APIPromise<ChatCompletionsResponse>;
  completions(
    body: ChatCompletionsParams & { stream: true },
    options?: RequestOptions,
  ): APIPromise<Stream<ChatCompletionChunk>>;
  completions(
    body: ChatCompletionsParams,
    options?: RequestOptions,
  ): APIPromise<ChatCompletionsResponse> | APIPromise<Stream<ChatCompletionChunk>> {
    const promise = this._client.post('/api/v1/chat/completions', { body, ...options });

    // Custom code: If streaming, return a Stream instead of parsed response
    if (body.stream) {
      const controller = new AbortController();
      // Access responsePromise through type assertion (it's private but we need it)
      const responsePromise = (promise as any).responsePromise as Promise<APIResponseProps>;
      return new APIPromise<Stream<ChatCompletionChunk>>(
        this._client,
        responsePromise,
        async (client, props: APIResponseProps) => {
          const response = props.response;
          return Stream.fromSSEResponse<ChatCompletionChunk>(response, controller, this._client);
        },
      );
    }

    return promise as APIPromise<ChatCompletionsResponse>;
  }
}

export interface ChatCompletionsResponse {
  /**
   * A unique identifier for this chat completion response. Can be used for tracking
   * or debugging.
   */
  id: string;

  /**
   * An array of completion choices. Each choice represents a possible completion for
   * the input prompt, though currently only one choice is typically returned.
   */
  choices: Array<ChatCompletionsResponse.Choice>;

  /**
   * The Unix timestamp (in seconds) indicating when this completion was generated by
   * the API.
   */
  created: number;

  /**
   * The specific model used to generate this completion. This will be the model's
   * full identifier string.
   */
  model: string;

  /**
   * The type of object returned, always "chat.completion" for chat completion
   * responses.
   */
  object: 'chat.completion';

  /**
   * A unique identifier for the system state that generated this response. Useful
   * for tracking model behavior across requests.
   */
  system_fingerprint?: string | null;

  /**
   * Statistics about token usage for this request and response. May be omitted in
   * error cases or when not available.
   */
  usage?: ChatCompletionsResponse.Usage;
}

export namespace ChatCompletionsResponse {
  export interface Choice {
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';

    index: number;

    message: Choice.Message;
  }

  export namespace Choice {
    export interface Message {
      content: string | null;

      role: 'assistant' | 'tool';

      name?: string;

      tool_call_id?: string;

      tool_calls?: Array<Message.ToolCall>;
    }

    export namespace Message {
      export interface ToolCall {
        id: string;

        function: ToolCall.Function;

        type: 'function';
      }

      export namespace ToolCall {
        export interface Function {
          arguments: string;

          name: string;
        }
      }
    }
  }

  /**
   * Statistics about token usage for this request and response. May be omitted in
   * error cases or when not available.
   */
  export interface Usage {
    /**
     * The number of tokens generated in the completion response. This counts only the
     * tokens in the assistant's reply.
     */
    completion_tokens: number;

    /**
     * The number of tokens in the prompt/input. This includes all messages sent to the
     * API in the conversation history.
     */
    prompt_tokens: number;

    total_tokens: number;
  }
}

export interface ChatCompletionsParams {
  /**
   * An array of messages comprising the conversation so far. Must contain at least
   * one message. System messages are only allowed as the first message.
   */
  messages: Array<ChatCompletionsParams.Message>;

  /**
   * The identifier of the model to use for generating completions. This can be a
   * model ID or an alias.
   */
  model: string;

  /**
   * A value between -2.0 and 2.0 that penalizes new tokens based on their frequency
   * in the text so far. Higher values decrease the likelihood of the model repeating
   * the same tokens.
   */
  frequency_penalty?: number;

  /**
   * The maximum number of tokens to generate in the completion. If null, will use
   * the model's maximum context length. This is the maximum number of tokens that
   * will be generated.
   */
  max_completion_tokens?: number | null;

  /**
   * A value between -2.0 and 2.0 that penalizes new tokens based on whether they
   * appear in the text so far. Higher values increase the likelihood of the model
   * talking about new topics.
   */
  presence_penalty?: number;

  /**
   * Specifies the format of the model's output. Use "json_schema" to constrain
   * responses to valid JSON matching the provided schema.
   */
  response_format?: ChatCompletionsParams.ResponseFormat;

  /**
   * A seed value for deterministic sampling. Using the same seed with the same
   * parameters will generate the same completion.
   */
  seed?: number;

  /**
   * One or more sequences where the API will stop generating further tokens. Can be
   * a single string or an array of strings.
   */
  stop?: string | Array<string>;

  /**
   * If true, partial message deltas will be sent as server-sent events. Useful for
   * showing progressive generation in real-time.
   */
  stream?: boolean;

  /**
   * Controls randomness in the model's output. Values between 0 and 2. Lower values
   * make the output more focused and deterministic, higher values make it more
   * random and creative.
   */
  temperature?: number | null;

  /**
   * Controls how the model uses tools. "none" disables tools, "auto" lets the model
   * decide, or specify a particular tool configuration.
   */
  tool_choice?: 'none' | 'auto' | unknown;

  /**
   * A list of tools the model may call. Each tool has a specific function the model
   * can use to achieve specific tasks.
   */
  tools?: Array<unknown>;

  /**
   * An alternative to temperature for controlling randomness. Controls the
   * cumulative probability of tokens to consider. Lower values make output more
   * focused.
   */
  top_p?: number | null;
}

export namespace ChatCompletionsParams {
  export interface Message {
    /**
     * The role of the message sender. "system" is for system-level instructions,
     * "user" represents the end user, and "assistant" represents the AI model's
     * responses.
     */
    role: 'system' | 'user' | 'assistant' | 'tool';

    /**
     * The actual text content of the message. Note that this can be null in certain
     * cases, such as when the message contains tool calls or when specific roles don't
     * require content.
     */
    content?: string | null;

    /**
     * The name of the function to call, if any.
     */
    name?: string;

    /**
     * The ID of the tool call that this message is a response to (only for tool role
     * messages)
     */
    tool_call_id?: string;

    /**
     * Tool calls to be made by the assistant
     */
    tool_calls?: Array<Message.ToolCall>;
  }

  export namespace Message {
    export interface ToolCall {
      /**
       * A unique identifier for this tool call
       */
      id: string;

      function: ToolCall.Function;

      /**
       * The type of tool call
       */
      type: 'function';
    }

    export namespace ToolCall {
      export interface Function {
        /**
         * The arguments to pass to the function as a JSON string
         */
        arguments: string;

        /**
         * The name of the function to call
         */
        name: string;
      }
    }
  }

  /**
   * Specifies the format of the model's output. Use "json_schema" to constrain
   * responses to valid JSON matching the provided schema.
   */
  export interface ResponseFormat {
    json_schema: { [key: string]: unknown };

    type: 'text' | 'json_schema';
  }
}

export declare namespace Chat {
  export {
    type ChatCompletionsResponse as ChatCompletionsResponse,
    type ChatCompletionsParams as ChatCompletionsParams,
  };
}
