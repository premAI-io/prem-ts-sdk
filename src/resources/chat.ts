// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';
import { APIPromise } from '../core/api-promise';
import { RequestOptions } from '../internal/request-options';

export class Chat extends APIResource {
  /**
   * Create a chat completion (OpenAI compatible).
   */
  completions(body: ChatCompletionsParams, options?: RequestOptions): APIPromise<ChatCompletionsResponse> {
    return this._client.post('/api/v1/chat/completions', { body, ...options });
  }
}

export interface ChatCompletionsResponse {
  /**
   * A unique identifier for this chat completion response. Can be used for tracking
   * or debugging.
   */
  id: string;

  /**
   * An array of completion choices. Each choice represents a possible completion for
   * the input prompt, though currently only one choice is typically returned.
   */
  choices: Array<ChatCompletionsResponse.Choice>;

  /**
   * The Unix timestamp (in seconds) indicating when this completion was generated by
   * the API.
   */
  created: number;

  /**
   * The specific model used to generate this completion. This will be the model's
   * full identifier string.
   */
  model: string;

  /**
   * The type of object returned, always "chat.completion" for chat completion
   * responses.
   */
  object: 'chat.completion';

  /**
   * A unique identifier for the system state that generated this response. Useful
   * for tracking model behavior across requests.
   */
  system_fingerprint?: string | null;

  /**
   * Statistics about token usage for this request and response. May be omitted in
   * error cases or when not available.
   */
  usage?: ChatCompletionsResponse.Usage;
}

export namespace ChatCompletionsResponse {
  export interface Choice {
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';

    index: number;

    message: Choice.Message;
  }

  export namespace Choice {
    export interface Message {
      content: string | null;

      role: 'assistant';
    }
  }

  /**
   * Statistics about token usage for this request and response. May be omitted in
   * error cases or when not available.
   */
  export interface Usage {
    /**
     * The number of tokens generated in the completion response. This counts only the
     * tokens in the assistant's reply.
     */
    completion_tokens: number;

    /**
     * The number of tokens in the prompt/input. This includes all messages sent to the
     * API in the conversation history.
     */
    prompt_tokens: number;

    total_tokens: number;
  }
}

export interface ChatCompletionsParams {
  /**
   * An array of messages comprising the conversation so far. Must contain at least
   * one message. System messages are only allowed as the first message.
   */
  messages: Array<ChatCompletionsParams.Message>;

  /**
   * The identifier of the model to use for generating completions. This can be a
   * model ID or an alias.
   */
  model: string;

  /**
   * A value between -2.0 and 2.0 that penalizes new tokens based on their frequency
   * in the text so far. Higher values decrease the likelihood of the model repeating
   * the same tokens.
   */
  frequency_penalty?: number;

  /**
   * The maximum number of tokens to generate in the completion. If null, will use
   * the model's maximum context length. This is the maximum number of tokens that
   * will be generated.
   */
  max_completion_tokens?: number | null;

  /**
   * A value between -2.0 and 2.0 that penalizes new tokens based on whether they
   * appear in the text so far. Higher values increase the likelihood of the model
   * talking about new topics.
   */
  presence_penalty?: number;

  /**
   * Specifies the format of the model's output. Use "json_schema" to constrain
   * responses to valid JSON matching the provided schema.
   */
  response_format?: ChatCompletionsParams.ResponseFormat;

  /**
   * A seed value for deterministic sampling. Using the same seed with the same
   * parameters will generate the same completion.
   */
  seed?: number;

  /**
   * One or more sequences where the API will stop generating further tokens. Can be
   * a single string or an array of strings.
   */
  stop?: string | Array<string>;

  /**
   * If true, partial message deltas will be sent as server-sent events. Useful for
   * showing progressive generation in real-time.
   */
  stream?: boolean;

  /**
   * Controls randomness in the model's output. Values between 0 and 2. Lower values
   * make the output more focused and deterministic, higher values make it more
   * random and creative.
   */
  temperature?: number | null;

  /**
   * Controls how the model uses tools. "none" disables tools, "auto" lets the model
   * decide, or specify a particular tool configuration.
   */
  tool_choice?: 'none' | 'auto' | unknown;

  /**
   * A list of tools the model may call. Each tool has a specific function the model
   * can use to achieve specific tasks.
   */
  tools?: Array<unknown>;

  /**
   * An alternative to temperature for controlling randomness. Controls the
   * cumulative probability of tokens to consider. Lower values make output more
   * focused.
   */
  top_p?: number | null;
}

export namespace ChatCompletionsParams {
  export interface Message {
    /**
     * The actual text content of the message. Note that this can be null in certain
     * cases, such as when the message contains tool calls or when specific roles don't
     * require content.
     */
    content: string;

    /**
     * The role of the message sender. "system" is for system-level instructions,
     * "user" represents the end user, and "assistant" represents the AI model's
     * responses.
     */
    role: 'system' | 'user' | 'assistant';
  }

  /**
   * Specifies the format of the model's output. Use "json_schema" to constrain
   * responses to valid JSON matching the provided schema.
   */
  export interface ResponseFormat {
    json_schema: { [key: string]: unknown };

    type: 'text' | 'json_schema';
  }
}

export declare namespace Chat {
  export {
    type ChatCompletionsResponse as ChatCompletionsResponse,
    type ChatCompletionsParams as ChatCompletionsParams,
  };
}
